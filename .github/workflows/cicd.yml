name: build

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build-and-release:

    runs-on: ubuntu-latest
    env:
      DEPLOYMENT_PATH: dbfs:/FileStore/jars/pyspark-craftsmanship
      DATABRICKS_URL: https://adb-984752964297111.11.azuredatabricks.net
      NOTEBOOK_PATH: /Users/himanshu.arora@databricks.com/covid/data_pipeline_notebook
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python 3.7
      uses: actions/setup-python@v2
      with:
        python-version: 3.7
    - uses: actions/setup-java@v1
      with:
        java-version: '11'
    - name: Install dependencies
      run: |
        sudo apt install python3-pip
        python3 -m pip install poetry
        poetry install
    - name: Configure Databricks CLI
      env:
        db_host: ${{ env.DATABRICKS_URL }}
        db_token: ${{ secrets.DATABRICKS_TOKEN }}
      run: |
        echo "${db_host}
        $db_token" | poetry run databricks configure --token
    - name: Test with pytest
      run: |
        poetry run pytest tests -v --junitxml="result.xml"
    - name: Publish Unit Test Results
      uses: EnricoMi/publish-unit-test-result-action@v1
      if: always()
      with:
        files: result.xml
    - name: Build package
      run: |
        poetry build
    - name: Deploy artifact
      run: |
        poetry run databricks fs cp ./dist $DEPLOYMENT_PATH -r --overwrite
    - name: Import notebook to workspace
      run: |
        poetry run databricks workspace import --language PYTHON --overwrite ./data_pipeline_notebook.py $NOTEBOOK_PATH
    - name: Create job
      run: |
        poetry run databricks jobs create --json '{
          "name": "Covid analysis pipeline",
          "new_cluster": {
            "spark_version": "8.2.x-scala2.12",
            "node_type_id": "Standard_DS3_v2",
            "num_workers": null,
            "autoscale": {
                "min_workers": 2,
                "max_workers": 8
            },
            "spark_env_vars": {
                "ENV_FOR_DYNACONF": "production"
            },
             "spark_conf": {
              "fs.azure.account.key.himanshudemostoageacc.dfs.core.windows.net": "${{ secrets.ADLS_DEMO_STORAGE_ACC_ACCESS_KEY }}"
            }
          },
         "libraries": [
            {
              "whl": "${{ env.DEPLOYMENT_PATH }}/jobs-0.1.0-py3-none-any.whl"
            }
          ],
          "notebook_task": {
            "notebook_path": "${{ env.NOTEBOOK_PATH }}"
          }
        }'
